{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilNfza8CpYnU"
   },
   "source": [
    "# INTELIGENCIA ARTIFICIAL: EB Ejercicios Tema 5: REDES NEURONALES \n",
    "## EJERCICIO: ¿Cómo calcular el número óptimo de neuronas en la capa oculta?\n",
    "**Implementar** una red neuronal para clasificar emails en SPAM/no SPAM con una única capa oculta. El número de neuronas de la capa oculta se debe calcular de forma óptima con una búsqueda grid usando para ello un conjunto de validación. Use como método de evaluación holdout, donde el conjunto de entrenamiento está formado por el 70% de las primeras filas del conjunto de datos, y el conjunto de test por el resto. \n",
    "\n",
    "Después de un preprocesado de los emails, cada email se codifica como un vector de 1899 elementos, donde cada elemento es una característica concreta del email. Todos los emails se encuentran repartidos en los ficheros **spamTrain.mat**, **spamVal.mat** y **spamTest.mat**, en los que tenéis la matriz de atributos **X** y el vector de clase **y** correspondiente al conjunto de training, de validación y de test, respectivamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TA1FENuDrZ8V"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SU5E_gnL-RC4"
   },
   "source": [
    "# A) DECLARACIÓN DE FUNCIONES Y CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB6G7rqop1sk"
   },
   "source": [
    "## 1) Cargar los datos de entrada\n",
    "Estos datos están almacenados en ficheros spamTrain.mat, spamVal.mat y spamTest.mat. \n",
    "Visualizar las dimensiones de la matriz de atributos y del vector clase para cada conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjyTyqjLpzQF",
    "outputId": "c7671f89-33f2-4e79-c293-a46ff2f57f0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data ...\n",
      "\n",
      "El tamaño de X_train es:  3000  filas y  1899  columnas.\n",
      "El tamaño de y_train es:  3000  filas y  1  columnas. \n",
      "El tamaño de X_val es:  1000  filas y  1899  columnas.\n",
      "El tamaño de y_val es:  1000  filas y  1  columnas. \n",
      "El tamaño de X_test es:  1000  filas y  1899  columnas.\n",
      "El tamaño de y_test es:  1000  filas y  1  columnas. \n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Data ...\\n\")\n",
    "\n",
    "# Datos de entrenamiento\n",
    "data_train = sio.loadmat(\"spamTrain.mat\") \n",
    "X_train = data_train['X']\n",
    "y_train = data_train['y']\n",
    "print(\"El tamaño de X_train es: \", X_train.shape[0], \" filas y \", X_train.shape[1], \" columnas.\")\n",
    "print(\"El tamaño de y_train es: \", y_train.shape[0], \" filas y \", y_train.shape[1], \" columnas. \")\n",
    "\n",
    "# Datos de validación\n",
    "data_val = sio.loadmat(\"spamValidation.mat\") \n",
    "X_val = data_val['X']\n",
    "y_val = data_val['y']\n",
    "print(\"El tamaño de X_val es: \", X_val.shape[0], \" filas y \", X_val.shape[1], \" columnas.\")\n",
    "print(\"El tamaño de y_val es: \", y_val.shape[0], \" filas y \", y_val.shape[1], \" columnas. \")\n",
    "\n",
    "# Datos de test\n",
    "data_test = sio.loadmat(\"spamTest.mat\") \n",
    "X_test = data_test['Xtest']\n",
    "y_test = data_test['ytest']\n",
    "print(\"El tamaño de X_test es: \", X_test.shape[0], \" filas y \", X_test.shape[1], \" columnas.\")\n",
    "print(\"El tamaño de y_test es: \", y_test.shape[0], \" filas y \", y_test.shape[1], \" columnas. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_QxZU8psVIV"
   },
   "source": [
    "## 2) Hipótesis y función coste\n",
    "El objetivo de la red neuronal es minimizar la función de coste definida por: \n",
    "\\begin{equation}\n",
    "J(\\Theta)=\\frac{-1}{m}\\sum_{i=1}^{m} \\sum_{k=1}^{K} (y_k^i \\cdot log((h_{\\theta}(x^i))_k)+(1-y_k^i)\\cdot log(1-(h_{\\theta}(x^i))_k))\n",
    "\\end{equation}\n",
    "\n",
    "Con el objeto de usar funciones de optimización avanzada de Python, implementar una función que devuelva el valor del coste.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bPREa9_8tVJZ"
   },
   "outputs": [],
   "source": [
    "# Función sigmoide tradicional\n",
    "def sigmoid(z):\n",
    "  g = 1 / (1+np.exp(-z))\n",
    "  return g\n",
    "\n",
    "# Función propagación hacia delante o forward propagation\n",
    "def forward(theta1, theta2, X, i):\n",
    "  # bias  + neuronas de la capa 1\n",
    "  a1 = np.hstack((1, X[i])) # Igual que: ones = np.ones(1)  a1 = np.hstack((ones, X[i]))\n",
    "  a2 = sigmoid(theta1 @ a1 )\n",
    "\n",
    "  # bias + neuronas de la capa 2\n",
    "  a2 = np.hstack((1, a2)) # Igual que: a2 = np.hstack((ones, a2))\n",
    "  \n",
    "  # a3 es la salida de la capa 3 o hipótesis (h)\n",
    "  a3 = sigmoid(theta2 @ a2)\n",
    "  return a1, a2, a3\n",
    "\n",
    "# Función coste sin regularizar para redes neuronales\n",
    "def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y):\n",
    "  # Paso 1: Enrollar nn_params para obtener cada uno de los theta (pesos/parámetros)\n",
    "  theta1 = np.reshape(a = nn_params[:hidden_layer_size*(input_layer_size+1)], # datos \n",
    "                      newshape = (hidden_layer_size, input_layer_size+1), # nueva shape: (hidden_layer_size, input_layer_size+1)\n",
    "                      order = 'F') # El order debe ser el mismo que cuando desenrollemos\n",
    "  theta2 = np.reshape(a = nn_params[hidden_layer_size*(input_layer_size+1):],\n",
    "                      newshape = (num_labels, hidden_layer_size+1), \n",
    "                      order = 'F')\n",
    "  \n",
    "  # Paso 2: Definir variables necesarias\n",
    "  m = len(y) \n",
    "  suma = 0 \n",
    "  y_d = pd.DataFrame(y) # ¡¡IMPORTANTE!!: No aplicamos one-hot encoding (y_d = pd.get_dummies(y.flatten())) ya que solo tenemos 1 clase: spam/no spam. \n",
    "  # Pero es importante transformar y a DataFrame para poder acceder fila por fila\n",
    "\n",
    "  # Paso 3: Para cada fila \n",
    "  for i in range(X.shape[0]):\n",
    "      # Paso 3.1: Forward propagation\n",
    "      a1, a2, h = forward(theta1, theta2, X, i)\n",
    "      # Paso 3.2: Coste (como en regresión logística)\n",
    "      temp1 = y_d.iloc[i] * (np.log(h)) \n",
    "      temp2 = (1 - y_d.iloc[i]) * np.log(1 - h) \n",
    "      temp3 = np.sum(temp1 + temp2) \n",
    "      suma = suma + temp3\n",
    "  J = (np.sum(suma) / (-m))\n",
    "  return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94KvljhWsvMw"
   },
   "source": [
    "## 3) Gradiente\n",
    "Para poder usar funciones de optimización avanzadas de Python, implementar una función que devuelva el valor del gradiente formado por las derivadas parciales. \n",
    "Recordar que las derivadas parciales se calculan con el algoritmo de Backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6LxexTmOt1KB"
   },
   "outputs": [],
   "source": [
    "def nnGradFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y):\n",
    "  # Paso 1: Enrollar nn_params para obtener cada uno de los theta (pesos/parámetros)\n",
    "  initial_theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                              (hidden_layer_size, input_layer_size + 1), 'F')\n",
    "  initial_theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):],\n",
    "                              (num_labels, hidden_layer_size + 1), 'F')\n",
    "  \n",
    "  # Paso 2: Definir variables necesarias\n",
    "  m = len(y)\n",
    "  y_d = pd.DataFrame(y) # ¡¡IMPORTANTE!!: No aplicamos one-hot encoding (y_d = pd.get_dummies(y.flatten())) ya que solo tenemos 1 clase: spam/no spam. \n",
    "  # Pero es importante transformar y a DataFrame para poder acceder fila por fila\n",
    "  delta1 = np.zeros(initial_theta1.shape) # Delta1 tendrá las mismas dimensiones que initial_theta1\n",
    "  delta2 = np.zeros(initial_theta2.shape) # Delta2 tendrá las mismas dimensiones que initial_theta2\n",
    "\n",
    "  # Paso 3: Para cada fila\n",
    "  for i in range(X.shape[0]):\n",
    "      # Paso 3.1: Forward propagation\n",
    "      a1, a2, a3 = forward(initial_theta1, initial_theta2, X, i)\n",
    "      # Paso 3.2: Cálculo de los delta/errores (capa 1 no tiene)\n",
    "      d3 = a3 - y_d.iloc[i] # última capa \n",
    "      d2 = np.multiply(np.dot(initial_theta2.T,d3), np.multiply(a2, 1-a2)) # capa 2\n",
    "      # Paso 3.3: Cálculo de las derivadas ajustando las dimensiones de los errores y las activaciones de cada capa correctamente\n",
    "      delta1 = delta1 + (np.reshape(d2[1:,],(hidden_layer_size, 1)) @ np.reshape(a1, (1, input_layer_size+1))) # IGUAL: delta1 = delta1 + d2[1:,np.newaxis] @ a1[np.newaxis, :] \n",
    "      delta2 = delta2 + (np.reshape(d3.values, (num_labels, 1)) @ np.reshape(a2, (1, hidden_layer_size+1))) # IGUAL: delta2 = delta2 + d3[:,np.newaxis] @ a2[np.newaxis, :] \n",
    "\n",
    "  # Paso 4: Se desenrollan ambas derivadas con el mismo order con el que se enrollaron\n",
    "  delta1 /= m\n",
    "  delta2 /= m\n",
    "  gradiente = np.hstack((delta1.ravel(order='F'), delta2.ravel(order='F')))\n",
    "  return gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxagKJQ-usRu"
   },
   "source": [
    "## 4) Función para entrenar la red neuronal usando optimizador avanzado\n",
    "Implementar una función llamada training que sea la encargada de realizar el entrenamiento de una red neuronal. Para ello, esta función recibirá los parámetros theta iniciales, el tamaño de la capa de entrada, el tamaño de la capa oculta, el número de clases y el conjunto con el que se realice el entrenamiento (X e y). Devolverá los parámetros theta óptimos que definen el modelo aprendido.  \n",
    "\n",
    "Se recomienda el uso de la función fmin_cg de la librería scipy.optimize. Esta función tiene como argumentos de salida el vector nn_params que contiene los pesos óptimos Theta1 y Theta 2 en forma de vector y el valor J alcanzado. \n",
    "\n",
    "Recordar que a partir del vector nn_params hay que reconstruir los pesos Theta1 y Theta2 con las dimensiones adecuadas, usando la función de Python reshape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4EukrtIovajs"
   },
   "outputs": [],
   "source": [
    "def training(initial_theta1, initial_theta2, X_train, y_train, input_layer_size, hidden_layer_size, num_labels):\n",
    "  maxiter = 10 # Si tarda demasiado, se puede bajar el número de iteraciones al hacer la prueba inicial para comprobar que el entrenamiento es el adecuado\n",
    "\n",
    "  # Paso 1: Desenrollar los parámetros con el mismo order con el que se enrollaron\n",
    "  nn_initial_params = np.hstack((initial_theta1.ravel(order='F'), initial_theta2.ravel(order='F')))\n",
    "\n",
    "  # Paso 2: Llamada al optimizador avanzado gradiente conjugado con la función: fmin_cg\n",
    "  nn_params = opt.fmin_cg(maxiter=maxiter, f=nnCostFunction, x0=nn_initial_params, fprime=nnGradFunction,\n",
    "                        args=(input_layer_size, hidden_layer_size, num_labels, X_train, y_train.flatten()), gtol = 0.005)\n",
    "  \n",
    "  # Paso 3: Enrollar los pesos/parámetros theta1 y theta2 desde la salida del optimizador avanzado (nn_params)\n",
    "  theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                      (hidden_layer_size, input_layer_size + 1), order = 'F')\n",
    "  theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):], \n",
    "                      (num_labels, hidden_layer_size + 1), order = 'F')\n",
    "  return theta1, theta2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9DXPJU5tEH1"
   },
   "source": [
    "## 5) Función para predecir\n",
    "Implementar una función para predecir si un conjunto de emails son SPAM/no SPAM (binario).\n",
    "\n",
    "Podrías usar directamente la función forward (fila por fila) y añadir la última línea que calcula la predicción {0,1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wf6tUGkKpQEH"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR\n",
    "def predict(theta1, theta2, X):\n",
    "  # Variables útiles\n",
    "  m = len(X)\n",
    "  ones = np.ones((m,1))\n",
    "\n",
    "  a1 = np.hstack((ones, X))\n",
    "  a2 = sigmoid(a1 @ theta1.T)\n",
    "  a2 = np.hstack((ones, a2))\n",
    "  h = sigmoid(a2 @ theta2.T) # La hipótesis o predicción \n",
    "\n",
    "  # Si h es mayor o igual que 0.5, entonces la predicción será 1, si es menor que 0.5 será 0\n",
    "  # Función np.where: https://numpy.org/doc/stable/reference/generated/numpy.where.html : np.where(condicion, value if TRUE, value if FALSE)\n",
    "  pred = np.where( h >= 0.5 , 1 , 0)\n",
    "  return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mC3mWPXCwWeF"
   },
   "source": [
    "## 6) Cálculo del número óptimo de neuronas de la capa oculta con búsqueda grid usando el conjunto de validación.\n",
    "La función optimalHiddenNeurons se encarga de calcular el número óptimo de neuronas en la capa oculta con una búsqueda grid usando un conjunto de validación. \n",
    "\n",
    "Para ello, esta función recibirá el tamaño de la capa de entrada, el conjunto con el que se realice el entrenamiento, y el conjunto con el que se calcule la tasa de acierto, y devolverá el número óptimo de neuronas. El número óptimo de neuronas será aquel que haga máxima la tasa de acierto. \n",
    "\n",
    "El grid estará formado por el conjunto de neuronas {1,2,3,4,5,6,7,8,9,10}.\n",
    "\n",
    "En esta función se imprimirá la tasa de acierto para el grid de neuronas, el número óptimo de neuronas y la tasa de acierto máxima alcanzada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGcWfufO7aMC"
   },
   "source": [
    "### 6.1) Función para inicializar los pesos aleatoriamente de una capa \n",
    "Esta función recibe como parámetro el peso de una capa con L_in neuronas de entrada y L_out neuronas de salida. [EB TEMA 5 PARTE II ÚLTIMA DIAPOSITIVA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zWwOS7U4zTWK"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR\n",
    "def randInitializeWeights(L_in, L_out):\n",
    "  # Variable a devolver tendrá dimensiones: (L_out, L_in+1) # +1 procedente de la bias\n",
    "  W = np.zeros((L_out, 1+L_in))\n",
    "\n",
    "  # Se va a inicializar W de manera random para \"romper\" la simetría mientras se entrena la red neuronal\n",
    "  epsilon_init = 0.12 # Se define un epsilon\n",
    "  W = np.random.rand(L_out, L_in+1) * 2 *  epsilon_init - epsilon_init\n",
    "  return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ByNWcT0vyaXf"
   },
   "outputs": [],
   "source": [
    "# COMPLETAR\n",
    "def optimalHiddenNeurons(input_layer_size, num_labels, X_train, y_train, X_val, y_val):\n",
    "  # Paso 1: # Inicializamos variables útiles\n",
    "  num_max_neuronas = 3 # el número máximo de neuronas en el grid\n",
    "  print('\\nCalculando número óptimo de neuronas de la capa oculta... \\n')\n",
    "  print('\\nNúmero máximo de neuronas: ',num_max_neuronas)\n",
    "\n",
    "  arr_accuracy = [] # Inicializamos la lista donde almacenaremos la precisión del conjunto de \n",
    "  # validación para los diferentes números de neuronas del grid\n",
    "\n",
    "  # Paso 2: Bucle desde 1 hasta num_max_neuronas (incluido, por eso el +1)\n",
    "  for hidden_layer_size in range(1, num_max_neuronas+1):\n",
    "    print('-----\\nNúmero de neuronas de la capa oculta: ',hidden_layer_size)\n",
    "\n",
    "    # Paso 2.1: Inicializar los pesos aleatoriamente con las dimensiones correctas\n",
    "    initial_theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n",
    "    initial_theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n",
    "\n",
    "    # Paso 2.2: Entrenamiento de la red neuronal con el número de neuronas de la capa oculta\n",
    "    theta1_opt, theta2_opt = training(initial_theta1, initial_theta2, X_train, y_train, input_layer_size, hidden_layer_size, num_labels)\n",
    "\n",
    "    # Paso 2.3: Predicción usando el conjunto de validación\n",
    "    pred = predict(theta1_opt , theta2_opt, X_val)\n",
    "    \n",
    "    # Paso 2.4: Calcular la precisión/accuracy máxima\n",
    "    arr_accuracy.append(np.mean(pred== y_val )) # Se añade a la lista de precisión\n",
    "    print(\"accuracy: \", np.mean(pred== y_val ))\n",
    "    \n",
    "  # Paso 3: Fuera del bucle, encontrar el número de neuronas ocultas con las que se consigue el mejor accuracy\n",
    "  optimal_hidden_layer_size = np.argmax(arr_accuracy)+1 # +1 porque np.argmax() nos proporciona la posición en la lista del mejor valor. Las posiciones empiezan en 0 y nosotros empezamos en 1 neurona\n",
    "  print(\"\\n**** El número de neuronas de la capa oculta óptimo es: \", optimal_hidden_layer_size)\n",
    "  print(\"**** Con esas neuronas en la capa oculta, el accuracy del conjunto de validación es: \", max(arr_accuracy))\n",
    "\n",
    "  return optimal_hidden_layer_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_DIBz9I-d5W"
   },
   "source": [
    "# B) EJECUCIÓN DEL PROBLEMA BUSCANDO EL NÚMERO ÓPTIMO DE NEURONAS DE LA CAPA OCULTA Y PREDICIENDO SOBRE EL CONJUNTO DE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2gcQTttHKkED",
    "outputId": "fb3e0a07-428f-4b23-da28-6b7c45a033c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculando número óptimo de neuronas de la capa oculta... \n",
      "\n",
      "\n",
      "Número máximo de neuronas:  3\n",
      "-----\n",
      "Número de neuronas de la capa oculta:  1\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.140893\n",
      "         Iterations: 7\n",
      "         Function evaluations: 22\n",
      "         Gradient evaluations: 22\n",
      "accuracy:  0.953\n",
      "-----\n",
      "Número de neuronas de la capa oculta:  2\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.098913\n",
      "         Iterations: 8\n",
      "         Function evaluations: 28\n",
      "         Gradient evaluations: 28\n",
      "accuracy:  0.975\n",
      "-----\n",
      "Número de neuronas de la capa oculta:  3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.110434\n",
      "         Iterations: 6\n",
      "         Function evaluations: 20\n",
      "         Gradient evaluations: 20\n",
      "accuracy:  0.967\n",
      "\n",
      "**** El número de neuronas de la capa oculta óptimo es:  2\n",
      "**** Con esas neuronas en la capa oculta, el accuracy del conjunto de validación es:  0.975\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1900)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m X_train_completo \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(X_train, X_val) \u001b[38;5;66;03m# IMPORTANTE\u001b[39;00m\n\u001b[0;32m     15\u001b[0m y_train_completo \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(y_train, y_val) \u001b[38;5;66;03m# IMPORTANTE\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m theta1_opt, theta2_opt \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_theta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_theta2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_completo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_completo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_layer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimal_hidden_layer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Paso 5: Predecir usando el conjunto de test y calcular el error\u001b[39;00m\n\u001b[0;32m     19\u001b[0m pred \u001b[38;5;241m=\u001b[39m predict(theta1_opt, theta2_opt, X_test )\n",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(initial_theta1, initial_theta2, X_train, y_train, input_layer_size, hidden_layer_size, num_labels)\u001b[0m\n\u001b[0;32m      5\u001b[0m nn_initial_params \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((initial_theta1\u001b[38;5;241m.\u001b[39mravel(order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m), initial_theta2\u001b[38;5;241m.\u001b[39mravel(order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Paso 2: Llamada al optimizador avanzado gradiente conjugado con la función: fmin_cg\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m nn_params \u001b[38;5;241m=\u001b[39m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin_cg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnnCostFunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn_initial_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfprime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnnGradFunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                      \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_layer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_layer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Paso 3: Enrollar los pesos/parámetros theta1 y theta2 desde la salida del optimizador avanzado (nn_params)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m theta1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(nn_params[:hidden_layer_size \u001b[38;5;241m*\u001b[39m (input_layer_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)],\n\u001b[0;32m     13\u001b[0m                     (hidden_layer_size, input_layer_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), order \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\danit\\anaconda3\\envs\\entornoIA2425\\lib\\site-packages\\scipy\\optimize\\_optimize.py:1659\u001b[0m, in \u001b[0;36mfmin_cg\u001b[1;34m(f, x0, fprime, args, gtol, norm, epsilon, maxiter, full_output, disp, retall, callback, c1, c2)\u001b[0m\n\u001b[0;32m   1651\u001b[0m opts \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgtol\u001b[39m\u001b[38;5;124m'\u001b[39m: gtol,\n\u001b[0;32m   1652\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m'\u001b[39m: norm,\n\u001b[0;32m   1653\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m: epsilon,\n\u001b[0;32m   1654\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp,\n\u001b[0;32m   1655\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: maxiter,\n\u001b[0;32m   1656\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn_all\u001b[39m\u001b[38;5;124m'\u001b[39m: retall}\n\u001b[0;32m   1658\u001b[0m callback \u001b[38;5;241m=\u001b[39m _wrap_callback(callback)\n\u001b[1;32m-> 1659\u001b[0m res \u001b[38;5;241m=\u001b[39m _minimize_cg(f, x0, args, fprime, callback\u001b[38;5;241m=\u001b[39mcallback, c1\u001b[38;5;241m=\u001b[39mc1, c2\u001b[38;5;241m=\u001b[39mc2,\n\u001b[0;32m   1660\u001b[0m                    \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopts)\n\u001b[0;32m   1662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_output:\n\u001b[0;32m   1663\u001b[0m     retlist \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnfev\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnjev\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\danit\\anaconda3\\envs\\entornoIA2425\\lib\\site-packages\\scipy\\optimize\\_optimize.py:1723\u001b[0m, in \u001b[0;36m_minimize_cg\u001b[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, c1, c2, **unknown_options)\u001b[0m\n\u001b[0;32m   1720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1721\u001b[0m     maxiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x0) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[1;32m-> 1723\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1724\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1726\u001b[0m f \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun\n\u001b[0;32m   1727\u001b[0m myfprime \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[1;32mc:\\Users\\danit\\anaconda3\\envs\\entornoIA2425\\lib\\site-packages\\scipy\\optimize\\_optimize.py:288\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    284\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32mc:\\Users\\danit\\anaconda3\\envs\\entornoIA2425\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:166\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[1;32mc:\\Users\\danit\\anaconda3\\envs\\entornoIA2425\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danit\\anaconda3\\envs\\entornoIA2425\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danit\\anaconda3\\envs\\entornoIA2425\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[1;32mIn[3], line 38\u001b[0m, in \u001b[0;36mnnCostFunction\u001b[1;34m(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Pero es importante transformar y a DataFrame para poder acceder fila por fila\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Paso 3: Para cada fila \u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# Paso 3.1: Forward propagation\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     a1, a2, h \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Paso 3.2: Coste (como en regresión logística)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     temp1 \u001b[38;5;241m=\u001b[39m y_d\u001b[38;5;241m.\u001b[39miloc[i] \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39mlog(h)) \n",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(theta1, theta2, X, i)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(theta1, theta2, X, i):\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;66;03m# bias  + neuronas de la capa 1\u001b[39;00m\n\u001b[0;32m      9\u001b[0m   a1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((\u001b[38;5;241m1\u001b[39m, X[i])) \u001b[38;5;66;03m# Igual que: ones = np.ones(1)  a1 = np.hstack((ones, X[i]))\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m   a2 \u001b[38;5;241m=\u001b[39m sigmoid(\u001b[43mtheta1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma1\u001b[49m )\n\u001b[0;32m     12\u001b[0m   \u001b[38;5;66;03m# bias + neuronas de la capa 2\u001b[39;00m\n\u001b[0;32m     13\u001b[0m   a2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((\u001b[38;5;241m1\u001b[39m, a2)) \u001b[38;5;66;03m# Igual que: a2 = np.hstack((ones, a2))\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1900)"
     ]
    }
   ],
   "source": [
    "# Paso 1: Inicializar el tamaño de la capa de entrada y el número de clases del problema\n",
    "input_layer_size = 1899 # Número de columnas de los datos de entrada (X)\n",
    "num_labels = 1 # Spam/No spam\n",
    "\n",
    "# Paso 2: Calcular el número óptimo de neuronas en la capa oculta usando la función optimalHiddenNeurons\n",
    "optimal_hidden_layer_size = optimalHiddenNeurons(input_layer_size, num_labels, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Paso 3: Inicialización de pesos con el número óptimo de neuronas en la capa oculta\n",
    "initial_theta1 = randInitializeWeights(input_layer_size, optimal_hidden_layer_size)\n",
    "initial_theta2 = randInitializeWeights(optimal_hidden_layer_size, num_labels)\n",
    "\n",
    "# Paso 4: Entrenamiento de la red neuronal con el número de neuronas óptimo y obtener los pesos óptimos. \n",
    "# Recordar que el entrenamiento debe hacerse con el conjunto de training entero (X_train y X_val)\n",
    "X_train_completo = np.append(X_train, X_val) # IMPORTANTE\n",
    "y_train_completo = np.append(y_train, y_val) # IMPORTANTE\n",
    "theta1_opt, theta2_opt = training(initial_theta1, initial_theta2, X_train_completo, y_train_completo, input_layer_size, optimal_hidden_layer_size, num_labels)\n",
    "\n",
    "# Paso 5: Predecir usando el conjunto de test y calcular el error\n",
    "pred = predict(theta1_opt, theta2_opt, X_test )\n",
    "print(\"Accuracy del conjunto de test: \", np.mean(pred == y_test )) # Calcular la precisión/accuracy máxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "entornoIA2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
