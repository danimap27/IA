{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzcS-cASiKyJ"
   },
   "source": [
    "# INTELIGENCIA ARTIFICIAL: EB Ejercicios Tema 7: SISTEMAS DE RECOMENDACIÓN \n",
    "## EJERCICIO: Normalización de las valoraciones\n",
    "**Implementar** un sistema de recomendación de filtrado colaborativo como el visto en la EPD6. Después, **implementar** el algoritmo de normalización de las valoraciones ($mean$ $normalization$). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ygf9vItvwMZM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize as opt\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gANN2J41wMu3"
   },
   "source": [
    "# A) SISTEMA DE RECOMENDACIÓN BASADO EN FILTRADO COLABORATIVO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPEQVN4pyDl5"
   },
   "source": [
    "## 1) Cargar los datos de entrada\n",
    "Serán los mismos que los usados en la EPD6: \n",
    "*   \"ex8_movies.mat\": Valoraciones de las 1682 películas por los 943 usuarios.\n",
    "*   \"ex8_movieParams.mat\": Parámetros de un modelo pre-entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ek-zooTPho1x",
    "outputId": "7ddf6086-bcd6-44e3-efad-7982b12bc597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de películas:  1682  número de usuarios:  943\n",
      "\n",
      "Y contiene las puntuaciones/valoraciones de 1-5 de las n_i películas y los n_u usuarios.\n",
      "\t Y es un  <class 'numpy.ndarray'>  con dimensiones:  (1682, 943)\n",
      "\n",
      "R indica si existe o no valoración de un usuario para una película.\n",
      "\t R es un  <class 'numpy.ndarray'>  con dimensiones:  (1682, 943)\n",
      "\n",
      "Media de las valoraciones de la primera película(Toy Story):  3.8783185840707963 /5\n",
      "\n",
      "****    *****\n",
      "\n",
      "El número de características de los ítems (películas) es :  10\n",
      "\n",
      "X contiene las características preentrenadas basadas en el contenido de las películas.\n",
      "\t X es un  <class 'numpy.ndarray'>  con dimensiones:  (1682, 10)\n",
      "\n",
      "Theta contiene los parámetros preentrenados de preferencia de nuestros usuarios.\n",
      "\t Theta es un  <class 'numpy.ndarray'>  con dimensiones:  (10, 943)\n"
     ]
    }
   ],
   "source": [
    "movies = sio.loadmat(\"ex8_movies.mat\")\n",
    "Y = movies['Y'] \n",
    "R = movies['R']\n",
    "n_items = Y.shape[0]\n",
    "n_users  = Y.shape[1]\n",
    "print(\"Número de películas: \", Y.shape[0], \" número de usuarios: \", Y.shape[1])\n",
    "#print(\"Número de películas: \", R.shape[0], \" número de usuarios: \", R.shape[1]) # Sería lo mismo que la sentencia anterior\n",
    "\n",
    "print(\"\\nY contiene las puntuaciones/valoraciones de 1-5 de las n_i películas y los n_u usuarios.\")\n",
    "print(\"\\t Y es un \", type(Y), \" con dimensiones: \", Y.shape)\n",
    "print(\"\\nR indica si existe o no valoración de un usuario para una película.\")\n",
    "print(\"\\t R es un \", type(R), \" con dimensiones: \", R.shape)\n",
    "\n",
    "print('\\nMedia de las valoraciones de la primera película(Toy Story): ', Y[0, np.where(R[0, :] == 1)[0]].mean(), \"/5\\n\")\n",
    "\n",
    "###\n",
    "\n",
    "params_data = sio.loadmat(\"ex8_movieParams.mat\")\n",
    "X = params_data['X']\n",
    "Theta = params_data['Theta']\n",
    "Theta = Theta.T\n",
    "features = X.shape[1] # Sería lo mismo que Theta.shape[0]\n",
    "print(\"****    *****\\n\\nEl número de características de los ítems (películas) es : \", features)\n",
    "\n",
    "print(\"\\nX contiene las características preentrenadas basadas en el contenido de las películas.\")\n",
    "print(\"\\t X es un \", type(X), \" con dimensiones: \", X.shape)\n",
    "print(\"\\nTheta contiene los parámetros preentrenados de preferencia de nuestros usuarios.\")\n",
    "print(\"\\t Theta es un \", type(Theta), \" con dimensiones: \", Theta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XOEYczjzfes"
   },
   "source": [
    "## 2) Función coste del filtrado colaborativo con regularización\n",
    "Usar el subconjunto de datos formado por los 4 primeros usuarios, 5 primeras películas y 3 primeros atributos y también para todos los datos del fichero \"ex8_movies.mat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xrXDbStmz12_"
   },
   "outputs": [],
   "source": [
    "def cofiCostFuncReg(params, Y, R, num_features, lambda_param):\n",
    "    num_movies, num_users = Y.shape\n",
    "    \n",
    "    # Desenrollar parámetros\n",
    "    X = np.reshape(params[:num_movies*num_features], (num_movies, num_features), 'F')\n",
    "    Theta = np.reshape(params[num_movies*num_features:], (num_features, num_users), 'F')\n",
    "    J = 0\n",
    "    \n",
    "    error = np.multiply(np.dot(X, Theta) - Y, R)\n",
    "    squared_error = np.power(error, 2)\n",
    "    J = 1/2 * np.sum(squared_error)\n",
    "    \n",
    "    # Regularización\n",
    "    J += lambda_param/2 * (np.sum(np.power(Theta, 2)) + (lambda_param/2)* np.sum(np.power(X, 2)))\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UKOxNidkzZDl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Para el subconjunto seleccionado J_reg debe ser cercano a 31.34:  30.0756100583271\n",
      "\n",
      "Para todos los datos J_reg debe ser cercano a 34821.70:  33958.21124286916\n"
     ]
    }
   ],
   "source": [
    "lambda_param = 1.5\n",
    "\n",
    "# Subconjunto de datos\n",
    "sub_users = 4\n",
    "sub_movies = 5\n",
    "sub_features = 3\n",
    "\n",
    "X_sub = X[:sub_movies, :sub_features]\n",
    "Theta_sub = Theta[:sub_features, :sub_users]\n",
    "Y_sub = Y[:sub_movies, :sub_users]\n",
    "R_sub = R[:sub_movies, :sub_users]\n",
    "\n",
    "params_sub = np.hstack((np.ravel(X_sub, order='F'), np.ravel(Theta_sub, order='F')))\n",
    "\n",
    "J_reg_sub = cofiCostFuncReg(params_sub, Y_sub, R_sub, sub_features, lambda_param)\n",
    "print(\"\\nPara el subconjunto seleccionado J_reg debe ser cercano a 31.34: \",J_reg_sub)\n",
    "\n",
    "# Todos los datos\n",
    "params = np.hstack((np.ravel(X, order='F'), np.ravel(Theta, order='F')))\n",
    "J_reg = cofiCostFuncReg(params, Y, R, features, lambda_param)\n",
    "print(\"\\nPara todos los datos J_reg debe ser cercano a 34821.70: \",J_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRQ7SquNzv8e"
   },
   "source": [
    "## 3) Gradiente del filtrado colaborativo con regularización\n",
    "Usar el subconjunto de datos formado por los 4 primeros usuarios, 5 primeras películas y 3 primeros atributos y también para todos los datos del fichero \"ex8_movies.mat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FHqMi0YAiJ8n"
   },
   "outputs": [],
   "source": [
    "def cofiGradientFuncReg(params, Y, R, num_features, lambda_param):\n",
    "    num_movies, num_users = Y.shape\n",
    "    \n",
    "    # Desenrollar parámetros\n",
    "    X = np.reshape(params[:num_movies*num_features], (num_movies, num_features), 'F')\n",
    "    Theta = np.reshape(params[num_movies*num_features:], (num_features, num_users), 'F')\n",
    "    \n",
    "    X_grad = np.zeros(X.shape)\n",
    "    Theta_grad = np.zeros(Theta.shape)\n",
    "    \n",
    "    # Coste\n",
    "    error = np.multiply(np.dot(X, Theta) - Y, R)\n",
    "    X_grad = np.dot(error, Theta.T)\n",
    "    Theta_grad = np.dot(X.T, error)\n",
    "    X_grad += lambda_param * X\n",
    "    Theta_grad += lambda_param * Theta\n",
    "    \n",
    "    # Desenrollar gradientes\n",
    "    grad = np.hstack((np.ravel(X_grad, order=\"F\"), np.ravel(Theta_grad,order=\"F\")))\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PjvKetFczW6i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradiente para el subconjunto seleccionado:  [ -0.95596339   0.60308088   0.12985616   0.29684395   0.60252677\n",
      "   6.97535514   2.77421145   4.0898522    1.06300933   4.90185327\n",
      "  -0.10861109   0.25839822  -0.89247334   0.66738144  -0.19747928\n",
      " -10.13985478   2.10136256  -6.76563628  -2.29347024   0.48244098\n",
      "  -2.99791422  -0.64787484  -0.71820673   1.27006666   1.09289758\n",
      "  -0.40784086   0.49026541]\n",
      "\n",
      "Gradiente para todos los datos:  [-4.68881319 -2.63803761 -2.16863787 ... -4.63438191  3.72934198\n",
      "  1.85226694]\n"
     ]
    }
   ],
   "source": [
    "lambda_param = 1.5\n",
    "\n",
    "# Subconjunto de datos\n",
    "sub_users = 4\n",
    "sub_movies = 5\n",
    "sub_features = 3\n",
    "\n",
    "X_sub = X[:sub_movies, :sub_features]\n",
    "Theta_sub = Theta[:sub_features, :sub_users]\n",
    "Y_sub = Y[:sub_movies, :sub_users]\n",
    "R_sub = R[:sub_movies, :sub_users]\n",
    "\n",
    "params_sub = np.hstack((np.ravel(X_sub, order='F'), np.ravel(Theta_sub, order='F')))\n",
    "\n",
    "grad_reg_sub = cofiGradientFuncReg(params_sub, Y_sub, R_sub, sub_features, lambda_param)\n",
    "print(\"\\nGradiente para el subconjunto seleccionado: \",grad_reg_sub)\n",
    "\n",
    "# Todos los datos\n",
    "params = np.hstack((np.ravel(X, order='F'), np.ravel(Theta, order='F')))\n",
    "grad_reg = cofiGradientFuncReg(params, Y, R, features, lambda_param)\n",
    "print(\"\\nGradiente para todos los datos: \",grad_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alFr5Q3W7LFP"
   },
   "source": [
    "## 4) Añadir nuevo usuario sin valoración\n",
    "Añadir un nuevo usuario sin valoración consiste en:\n",
    "*   Añadir al array $Y$ un nuevo array vacío con 1 columna y tantas filas como ítems tengamos. Usar $np.empty([n_{filas}, n_{columnas}])$.\n",
    "*   Añadir al array $R$ un nuevo array de ceros con 1 columna y tantas filas como ítems tengamos. Usar $np.zeros((n_{filas}, n_{columnas}))$.\n",
    "*   Aumentar el número de usuarios en 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "POpWs0f5IL4j"
   },
   "outputs": [],
   "source": [
    "# Añadir nuevo usuario del que no conocemos ninguna valoración\n",
    "null_array = np.zeros((n_items, 1))\n",
    "Y = np.append(Y, null_array, axis=1) # Append siempre añade al final del array. Axis=1 para añadir los valores como columna\n",
    "\n",
    "zeros_rating = np.zeros((n_items, 1))\n",
    "R = np.append(R, zeros_rating, axis=1) # Append siempre añade al final del array. Axis=1 para añadir los valores como columna\n",
    "\n",
    "n_users = n_users+1 # IMPORTANTE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2ff8MnpIO_j"
   },
   "source": [
    "## 5) Inicializar X y $\\Theta$ de forma random\n",
    "Inicializar de forma random con valores pequeños tanto la matriz X como la matriz Theta para todo el conjunto de datos. Usar la función $np.random.rand()$ indicando las dimensiones en los parámetros de entrada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "03pYSyPEIVXv"
   },
   "outputs": [],
   "source": [
    "# Inicializar Theta y X con valores random pequeños\n",
    "X = np.random.rand(n_items, features)\n",
    "Theta = np.random.rand(n_users, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqXjFXvlIWCY"
   },
   "source": [
    "## 6) Normalización de las valoraciones\n",
    "\n",
    "### **Normalización de las valoraciones en sistemas de recomendación:**###\n",
    "\n",
    "Cuando tengamos un nuevo usuario sin valoraciones (r(i,j)=0 para todos los ítems) ni preferencias, solo podremos minimizar en la función coste del filtrado colaborativo la parte correspondiente a la regularización de $\\Theta$. Por lo que los parámetros $\\Theta$ para el usuario $j$ acabarán siendo muy cercanos a 0. Y por tanto, todas las predicciones de valoración de los ítems del nuevo usuario serán cercanas a 0. \n",
    "\n",
    "Una solución a este problema es normalizar las valoraciones por película que tengamos. Pasos:\n",
    "1.   Crear array $Y_{mean}$ con dimensiones: [n$_i$, 1] donde para cada ítem (fila) tendremos la media de las valoraciones.\n",
    "2.   Crear array $Y_{norm}$ con dimensiones: [n$_i$, n$_u$] donde a la valoración de cada película (en $Y$) le restaremos su media correspondiente (en $Y_{mean}$). Si desconocemos la valoración, la dejaremos sin conocer.\n",
    "\\begin{equation}\n",
    "Y_{norm} = Y - Y_{mean}\n",
    "\\end{equation}\n",
    "3.   Haremos las predicciones de las valoraciones por ítem (usando como siempre la hipótesis) y le sumaremos la media correspondiente ($Y_{mean}$). Es decir, la predicción de la valoración del usuario $j$ al ítem $i$ sería:\n",
    "\\begin{equation}\n",
    "(\\Theta^{(j)})^{T} x^{i} + Y_{mean}^{i}\n",
    "\\end{equation}\n",
    "De forma vectorizada, las predicciones se pueden representar como:\n",
    "\\begin{equation}\n",
    " X · \\Theta + Y_{mean}\n",
    " \\end{equation}\n",
    "4.   Por tanto, si tenemos un usuario sin ninguna valoración, las predicciones de sus valoraciones serán muy cercanas a las medias.\n",
    "\n",
    "La normalización de las valoraciones puede usarse también como parte del preprocesado de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "23auZxfp6X3G"
   },
   "outputs": [],
   "source": [
    "def normalizacion (n_items, n_users, R, Y):\n",
    "  # Inicialización con ceros de Ymean y Ynorm con dimensiones adecuadas\n",
    "  Ymean = np.zeros((n_items, 1))\n",
    "  Ynorm = np.zeros((n_items, n_users))\n",
    "  # Para cada ítem\n",
    "  for i in range(n_items):\n",
    "      idx = np.where(R[i, :] == 1)[0]\n",
    "      Ymean[i] = np.mean(Y[i, idx])\n",
    "      Ynorm[i, idx] = Y[i, idx] - Ymean[i]\n",
    "  print(\"Mean Y matrix normalized: \", Ynorm.mean())\n",
    "  return Ymean, Ynorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2IVwU1XuzxUb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Y matrix normalized:  6.178285186614429e-19\n"
     ]
    }
   ],
   "source": [
    "# Normalización con usuario nuevo \n",
    "Ymean, Ynorm = normalizacion(n_items, n_users, R, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lURqCX1D9-G"
   },
   "source": [
    "## 7) Función optimizadora y predicción\n",
    "Usar la función optimizadora fmin_cg de la librería scipy.optimize. Usar las versiones regularizadas de la función coste y la función que calcula el gradiente. Utilizar el vector $Y$ normalizado y los parámetros $X$ y $\\Theta$ inicializados de forma random.  \n",
    "\n",
    "Una vez obtenidos los parámetros $X$ y $\\Theta$ que optimizan la función coste, calcular las predicciones considerando que hemos normalizado. Almacenar en la variable $my_{preds}$ las predicciones correspondientes al nuevo usuario que hemos añadido (como es el último del array, podemos acceder a él seleccionando la columna \"-1\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cOQ3ZE5T0nDv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: 23332.785419\n",
      "         Iterations: 300\n",
      "         Function evaluations: 441\n",
      "         Gradient evaluations: 441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danit\\anaconda3\\envs\\entornoIA2425\\lib\\site-packages\\scipy\\optimize\\_optimize.py:1659: OptimizeWarning: Maximum number of iterations has been exceeded.\n",
      "  res = _minimize_cg(f, x0, args, fprime, callback=callback, c1=c1, c2=c2,\n"
     ]
    }
   ],
   "source": [
    "# Desenrollar\n",
    "lambda_param = 0.1\n",
    "g_tol = 0.001\n",
    "params_rnd = np.hstack((np.ravel(X, order='F'), np.ravel(Theta, order='F')))\n",
    "\n",
    "# Función optimizadora\n",
    "fmin = opt.fmin_cg(gtol=g_tol, maxiter=300,f=cofiCostFuncReg, x0=params_rnd, fprime=cofiGradientFuncReg, args=(Ynorm, R, features, lambda_param))\n",
    "\n",
    "# Enrollar los parámetros optimizados\n",
    "X = np.reshape(fmin[:n_items*features], (n_items, features), order='F')\n",
    "Theta = np.reshape(fmin[n_items*features:], (n_users, features), order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ky8KnidJ-ZDa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.33434783, 3.92830918, 4.6608644 , ..., 3.99453985, 4.13754691,\n",
       "        3.90209303],\n",
       "       [3.79725389, 3.72180555, 4.71795211, ..., 3.60002316, 3.15648666,\n",
       "        2.22421654],\n",
       "       [2.96150795, 3.74753444, 3.94485256, ..., 4.22206009, 1.34541687,\n",
       "        2.726379  ],\n",
       "       ...,\n",
       "       [2.04861347, 2.04359759, 2.02963014, ..., 1.98381064, 2.03067833,\n",
       "        2.18050461],\n",
       "       [3.01440905, 3.06069226, 3.04645311, ..., 3.01925073, 3.01579319,\n",
       "        2.98960574],\n",
       "       [2.97372566, 3.06149023, 3.00177347, ..., 3.07479575, 3.02021226,\n",
       "        3.18317384]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([3.90209303, 2.22421654, 2.726379  , ..., 2.18050461, 2.98960574,\n",
       "       3.18317384])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predicciones con normalización\n",
    "predictions = np.dot(X, Theta.T) + Ymean\n",
    "display(predictions)\n",
    "\n",
    "# Me quedo solo con el último usuario: el que he añadido nuevo\n",
    "my_preds = predictions[:, -1]\n",
    "display(my_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpKWd4nMIkwr"
   },
   "source": [
    "## 8) Recomendamos las 10 películas con valoración predicha mayor para el nuevo usuario\n",
    "Leer los datos correspondientes a las películas desde \"movie_ids.txt\".\n",
    "Para recomendar las 10 películas con predicción de valoración más alta, usar la sentencia $np.argsort()$ que ordena de menor a mayor. Después para conseguir que ordene de mayor a menor usar $[::-1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "l1PVG53SBMqa"
   },
   "outputs": [],
   "source": [
    "# Leemos el fichero con los ids y nombres de las películas\n",
    "movie_idx = {}\n",
    "f = open('movie_ids.txt',encoding = 'ISO-8859-1')\n",
    "for line in f:\n",
    "    tokens = line.split(' ')\n",
    "    tokens[-1] = tokens[-1][:-1]\n",
    "    movie_idx[int(tokens[0]) - 1] = ' '.join(tokens[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "n1hyJGRb0zCH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 movie predictions:\n",
      "Predicted rating of 11.868202954983179 for movie Freeway (1996).\n",
      "Predicted rating of 11.69211802941491 for movie Burnt By the Sun (1994).\n",
      "Predicted rating of 11.690230383701408 for movie Bread and Chocolate (Pane e cioccolata) (1973).\n",
      "Predicted rating of 10.453704600621816 for movie Ponette (1996).\n",
      "Predicted rating of 9.925086921832772 for movie Primary Colors (1998).\n",
      "Predicted rating of 9.863801470114133 for movie Grace of My Heart (1996).\n",
      "Predicted rating of 9.852603778356864 for movie Big Blue, The (Grand bleu, Le) (1988).\n",
      "Predicted rating of 9.829490009347312 for movie Love Affair (1994).\n",
      "Predicted rating of 9.758863913035487 for movie Live Nude Girls (1995).\n",
      "Predicted rating of 9.597860125915398 for movie Strawberry and Chocolate (Fresa y chocolate) (1993).\n"
     ]
    }
   ],
   "source": [
    "idx = np.argsort(my_preds, axis=0)[::-1] # Ordenar por las predicciones de menor a mayor y coger sus índice. [::-1] significa que le damos la vuelta a la salida: de mayor a menor\n",
    "\n",
    "print(\"Top 10 movie predictions:\")\n",
    "# Imprimir las 10 películas con predicción de valoración del nuevo usuario más altas\n",
    "for i in range(10):\n",
    "      j = int(idx[i])\n",
    "      print('Predicted rating of {0} for movie {1}.'.format(str(float(my_preds[j])), movie_idx[j]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EB_EjerciciosTema7_MaterialAlumnos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "entornoIA2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
