# Tema 3: Regresi贸n Lineal

## Regresi贸n Lineal Univariable
- **Aprendizaje supervisado:** Se predice una salida que es un valor real (problema de regresi贸n).
- **Modelo:** Se utiliza una sola variable para predecir el valor de salida.

### Representaci贸n del Modelo
La hip贸tesis del modelo es:
    $\ h_\theta(x) = \theta_0 + \theta_1 x \$  
Donde:
-  es la variable de entrada.
-  $\ h_\theta(x)$ es la predicci贸n de la variable de salida.

### Funci贸n Coste
Para evaluar la precisi贸n del modelo, se utiliza la **funci贸n coste**, que mide el error entre los valores predichos y los valores reales.
La funci贸n coste se expresa como $J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x_i) - y_i)^2$.


### Descenso del Gradiente
El algoritmo de **descenso del gradiente** se utiliza para minimizar la funci贸n coste ajustando los valores de $\(\theta_0\) y \(\theta_1\).
\[
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)
\]$
Donde $\(\alpha\)$ es la tasa de aprendizaje.

---

## Regresi贸n Lineal Multivariable
En este caso, se utilizan m煤ltiples variables de entrada para predecir el valor de salida.

### Representaci贸n del Modelo
La hip贸tesis del modelo es:
$\[
h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \dots + \theta_nx_n
\]$

### Feature Scaling / Escalado de Variables
Es importante que todas las variables de entrada tengan una escala similar para que el descenso del gradiente converja r谩pidamente.

### Ecuaci贸n Normal
La **ecuaci贸n normal** ofrece una soluci贸n anal铆tica a la regresi贸n lineal sin necesidad de usar el descenso del gradiente:
$\[
\theta = (X^TX)^{-1}X^Ty
\]$

---

[Volver al 铆ndice](../README.md)

[Ir al siguiente tema: Tema 4: Regresi贸n Log铆stica](Tema4.md)